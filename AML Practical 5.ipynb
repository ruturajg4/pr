{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b38d7a-92c4-4a9e-a186-d1290a0cb168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define states and indices\n",
    "state_to_index = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "index_to_state = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E'}\n",
    "\n",
    "# Reward matrix\n",
    "reward_matrix = np.array([\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 1, 0, 1],\n",
    "    [0, 0, 0, 1, 0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "963710c3-c69d-43c9-8cb9-1119d406bafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Matrix:\n",
      "[[0 1 0 0 0]\n",
      " [1 0 1 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 1 0 1]\n",
      " [0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reward Matrix:\")\n",
    "print(reward_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26f5e4fd-cd7d-4596-9c04-dee9d4e05e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95     \n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80fb26dd-3450-4b6e-bd46-273a1a5dc828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discount Factor (Gamma): 0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"Discount Factor (Gamma):\", gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8dbaa91b-5547-4ac0-9f55-512f4b94e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(state_to_index)\n",
    "action_size = state_size\n",
    "Q_matrix = np.zeros((state_size, action_size))\n",
    "\n",
    "def q_learning_update(s, a, reward, s2, Q_matrix):\n",
    "    Q_matrix[s, a] = (1 - alpha) * Q_matrix[s, a] + alpha * (reward + gamma * np.max(Q_matrix[s2, :]))\n",
    "    return s2, Q_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ff751a3-abc2-4839-b5bc-01bedf0e1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, Q_matrix, epsilon=0.1):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(action_size)\n",
    "    return np.argmax(Q_matrix[state, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df1530e-3403-4d6e-8f8f-349357075e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_route(initial_state, goal_state, Q_matrix, episodes=1000):\n",
    "    for _ in range(episodes):\n",
    "        state = initial_state\n",
    "        while state != goal_state:\n",
    "            action = get_action(state, Q_matrix)\n",
    "            next_state = action\n",
    "            reward = reward_matrix[state, action]\n",
    "            state, Q_matrix = q_learning_update(state, action, reward, next_state, Q_matrix)\n",
    "    return Q_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4be347d6-5c63-4e10-a414-4296293bae5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-matrix:\n",
      "[[19.         20.         19.         18.05        0.        ]\n",
      " [20.         19.         20.         18.05        0.        ]\n",
      " [19.         20.         19.         19.05        0.        ]\n",
      " [16.54042605 19.         18.83342089 16.60985241  0.90152291]\n",
      " [ 0.          0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "initial_state = state_to_index['A']\n",
    "goal_state = state_to_index['E']\n",
    "Q_matrix = find_optimal_route(initial_state, goal_state, Q_matrix)\n",
    "\n",
    "print(\"\\nQ-matrix:\")\n",
    "print(Q_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78802731-5643-4d25-a630-efeb27c2914f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Actions for each state: ['B', 'A', 'B', 'B', 'A']\n"
     ]
    }
   ],
   "source": [
    "optimal_actions = [np.argmax(Q_matrix[state, :]) for state in range(state_size)]\n",
    "optimal_actions = [index_to_state[action] for action in optimal_actions]\n",
    "print(\"\\nOptimal Actions for each state:\", optimal_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f325c9e1-2bb5-43a1-a4c3-4d17f7ee62fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immediate Reward for moving from state A to state A: 0\n",
      "Immediate Reward for moving from state A to state B: 1\n",
      "Immediate Reward for moving from state A to state C: 0\n",
      "Immediate Reward for moving from state A to state D: 0\n",
      "Immediate Reward for moving from state A to state E: 0\n",
      "Immediate Reward for moving from state B to state A: 1\n",
      "Immediate Reward for moving from state B to state B: 0\n",
      "Immediate Reward for moving from state B to state C: 1\n",
      "Immediate Reward for moving from state B to state D: 0\n",
      "Immediate Reward for moving from state B to state E: 0\n",
      "Immediate Reward for moving from state C to state A: 0\n",
      "Immediate Reward for moving from state C to state B: 1\n",
      "Immediate Reward for moving from state C to state C: 0\n",
      "Immediate Reward for moving from state C to state D: 1\n",
      "Immediate Reward for moving from state C to state E: 0\n",
      "Immediate Reward for moving from state D to state A: 0\n",
      "Immediate Reward for moving from state D to state B: 0\n",
      "Immediate Reward for moving from state D to state C: 1\n",
      "Immediate Reward for moving from state D to state D: 0\n",
      "Immediate Reward for moving from state D to state E: 1\n",
      "Immediate Reward for moving from state E to state A: 0\n",
      "Immediate Reward for moving from state E to state B: 0\n",
      "Immediate Reward for moving from state E to state C: 0\n",
      "Immediate Reward for moving from state E to state D: 1\n",
      "Immediate Reward for moving from state E to state E: 0\n"
     ]
    }
   ],
   "source": [
    "for state in state_to_index:\n",
    "    for action in state_to_index:\n",
    "        state_idx = state_to_index[state]\n",
    "        action_idx = state_to_index[action]\n",
    "        immediate_reward = reward_matrix[state_idx, action_idx]\n",
    "        print(f\"Immediate Reward for moving from state {state} to state {action}: {immediate_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac87e31-1e88-41f9-9ad8-99c4a8acb157",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
